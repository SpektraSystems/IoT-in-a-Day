# Exercise 2: Processing Data from Azure IoT Hub 

## Scenario

Fabrikam, Inc.â€™s team requires that all temperature data should be available for real-time consumption and analysis along with long term storage for historical analysis and data retention. Fabrikam's management team requires data to be accessible in the forms of real-time dashboards and reports. 

In order to achieve the requirements, you will need to store all data generated by Turbine devices in long-term storage for the cold path analysis and transform a subset of the telemetry data for real-time analysis. Additionally, You will also need to configure visualization on the hot path dataset using PowerBI and TSI Explorer.

## Overview

In this exercise, you will use Stream Analytics to process and transform the data in IoT Hub. You will store all data in blob storage for Cold path analysis and filter a subset of the telemetry data for real-time analysis and visualization on the hot path in PowerBI.

This includes the following tasks:

* Review Stream Analytics Job
* Start Stream Analytics Job
* Review data in blob storage using Storage Explorer
* Visualize hot path data in PowerBI
* View Time Series Insights Explorer

## Solution architecture
 
   ![](./media/exercise2.png)      

### Task 1: Start Stream Analytics Job
**Azure Stream Analytics** is real-time analytics and complex event-processing engine that is designed to analyze and process high volumes of fast streaming data from multiple sources simultaneously. You can learn more about Azure Stream Analytics by following the links: https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-introduction

In this task, we will run a predefined **Stream Analytics Job** to process and transform the data from IoT Hub. You will send all the data to a storage account for long term storage so that Fabrikam Inc. can summarize it periodically. You will also send data to PowerBI service to visualize the "hot" data showing the average temperature reported by each device over a 1-minute window. 


1. Navigate to Azure Portal( https://portal.azure.com ). From the resource group **iot-{deployment-id}**, select the stream analytics job named **iot-streamjob-{deployment-id}**.

1. Then under Job Topology on the left side of the blade, select **Inputs**  and verify that there is one input job already defined with the input source set as IoT Hub. This input configures stream analytics to process each event being sent to IoT Hub.

   ![](./media/iothubinput.png)

1. Now, Let's review the outputs data stream configured for this Stream analytics Job. select **Outputs** under Job Topology. You will find 3 outputs configured.

 * **Blob output:** Stores the processed data in an Azure Blob Storage account. This output is configured to create separate folders based on the date and time.
 * **PowerBI output:** Connects the PowerBI workspace to access real-time data stream in Stream Analytics.
 * **Service Bus output:** Send the data to a Service Bus Namespace. Service bus output is used in the next exercise to configure Alerts.

1. Since you're logging in for the first time, You'll need to renew the authorization for PowerBI workspace.  Select **powerbioutput**  and click on **Renew authorization**. 

   ![](./media/credentials.png)

1. When prompted for Azure Credentials, provide the  **Azure Username** and **Password** from the environment details tab and click on **Save**.

    * Email/Username: <inject key="AzureAdUserEmail"></inject>
    * Password: <inject key="AzureAdUserPassword"></inject>

1. Navigate back to the **Overview** blade of the *Stream Analytics job*, now we will observe the query used to transform and analyze the data: 

       SELECT * INTO bloboutput FROM iothubinput; 
 
   > **Note**: This query selects all the telemetry data from the **iothubinput** and sends it to the **bloboutput**. For this basic query, we don't have to use Azure Stream Analytics Job, instead, we can use [**message routing**](https://docs.microsoft.com/en-us/azure/iot-hub/iot-hub-devguide-messages-d2c) functionality of Azure IoT Hub itself to store the data in a storage account. In Azure Stream Analytics we will be able to process the data and transform it as demonstrated in the next query.

       SELECT AVG(temp) AS Average, id  INTO powerbioutput  FROM iothubinput  GROUP BY TumblingWindow(minute, 1), id;

   > **Note**: This query looks at the events coming into the "iothubinput" Input and groups by a Tumbling Window of one minute. It sends the average of temperature and id to the "powerbioutput" Output, For more information about the `TumblingWindow` functions, reference this link: https://docs.microsoft.com/en-us/stream-analytics-query/tumbling-window-azure-stream-analytics

1. On the Overview page, click on **Start**.

1. In the **Start job** blade, select **Now** (the job will start processing messages from the now onwards) and click on **Start**.

1. It will take a few minutes for the Stream Analytics Job to get started and once started the Status on the overview blade will change to Running.
   
   ![](./media/iotstreamjob.png)

1. Once the Stream Analytics Job is successfully started, verify that It's a non-zero amount of **Input Events** on the **Monitoring** chart on the **Overview** blade. 

   ![](./media/monitor.png)
 
  > **Note:** If you don't see the data in a few minutes, you will need to reconnect your devices on the IoT Simulator by deactivating them first and then registering the devices again. Once registered, enable and start sending telemetry. 
  
### Task 2: Review data in blob storage using Storage Explorer

Azure Storage is a durable, highly available, massively scalable cloud storage solution. You can learn more about Azure Storage by following this link: https://azure.microsoft.com/en-in/services/storage/

In this task, we will review the data stored in the storage account **iotstorage{deployment-id}** as a result of the Azure Stream analytics job started in the previous task.

1. Navigate to the resource group iot-{deployment-id} and select the storage account named **iotstorage{deployment-id}**.

1. On the left-side menu, click **Storage Explorer (preview)**.

1. In **Storage Explorer (preview)**, under **BLOB CONTAINERS**, click **temperaturecontainer**.
 
1. In the right-hand pane, under **Name**, double-click the folder **output** to see the data. 

   ![](./media/storage_explorer.png)
   
1. Click on **Download** on under the **Storage Explorer (preview)** pane.

   ![](./media/download.png)
   
1. Then click on *"Click here to begin download"*.
   
   ![](./media/clickhere.png)

1. Review the contents of the JSON file you downloaded, it will contain entities such as **id**, **time**, **temp**, **EventProcessUtcTime** etc.

   ![](./media/data.png)
   
   You have now configured the solution to store the data in Azure Blob storage for long term storage. 
  
### Task 3: Visualize hot data with Power BI

**Power BI** is a collection of software services, apps, and connectors that work together to turn your unrelated sources of data into coherent, visually immersive, and interactive insights. You can learn more about Power BI by following the links: https://docs.microsoft.com/en-us/power-bi/fundamentals/power-bi-overview.

In this task, you will visualize the hot data being processed by Stream Analytics using PowerBI.
 
1. Sign in to your Power BI account (<https://app.powerbi.com>) using the lab credentials.

1. Select **My Workspace** on the left-hand menu and locate the **temperaturedataset** dataset from the list.

   > **Note:** It may take few minutes for the dataset to appear in the Power BI Dataset tab under **My Workspace**.

1. Select the ellipsis for more options on the right of the **temperaturedataset** and then select the **Create report** option from the dropdown list.
  
   ![](media/create_report.png)
   
1. On the **Visualizations** palette, select **Stacked column chart** to create a chart visualization.

   ![](media/visualization.png)

1. In the **Fields** listing, drag the **id** field, and drop it into the **Axis** field.

1. Next, drag the **average** field and drop it into the **Values** field.

   ![](media/average.png)

1. Now, set the **Value** to **Max of average**, by selecting the down arrow next to **average**, and select **Maximum**.

   ![](media/maximum.png)

1. Repeat steps 4-6, this time adding a Stacked Column Chart for **Min of average** by selecting the down arrow next to **average**, and select **Minimum**. 
   
   > **Note:** You may need to select on any area of white space on the report designer surface to deselect the Max of average by id chart visualization.

1. Next, add a **table visualization**.

   ![](media/table_visualization.png)

1. Set the values to **id** and **Average**, by dragging and dropping both fields in the **Values** field, then selecting the dropdown next to **average**, and selecting **Average** in the average field.

   ![](media/id.png)

1. Go to File, from the dropdown select **Save** to save the report.

   ![](media/filesave.png)

1. Enter the name `Average Temperatures`, and select **Save**.

1. Within the report, select one of the columns to see the data for a specific device. 

You have now configured PowerBI to visualize the Turbines data stream. 
   
### Task 4: View Time Series Insights Explorer 

**Azure Time Series Insights (TSI)** is an end-to-end **platform-as-a-service** offering used to collect, process, store, analyze, and query data from IoT solutions at scale. TSI is designed for ad hoc data exploration and operational analysis of data that are highly contextualized and optimized for time series. You can learn more about Time Series Insights by following the links: https://docs.microsoft.com/en-us/azure/time-series-insights/overview-what-is-tsi

For this task, Time Series Insights integration is already done with Azure IoT Hub and you will analyze the data in TSI Explorer. Since we only have data from the devices for the past ~20 minutes, It is not enough data to explore the **Time Series Insights**. In order to learn the capabilities of Time Series Insights, we will be using the built-in demo environment to explore TSI.

1. On the resource group tile, click **iot-{deployment-id}** and select the time series environment named **iot-tsi-{deployment-id}**.

1. On the **Time Series Insights environment** blade, at the top of the **Overview** pane, click **Go to TSI Explorer**.

   ![](media/gototsi.png)

   This will open the **Time Series Insights Explorer** in a new browser tab.

1. Once logged in to the TSI explorer pane, you should see time series instances for each of the devices you have registered.
  
   ![](media/tsi-turbine.png)
   
      > **Note:** It may take few minutes for the instances to appear. Please continue with the next steps even if you do not see the time series instances. 
  
1. Let's access the demo environment now, click on the user pane at the top right corner and select the **View Demo Environment** from the dropdown.

   ![](media/viewdemo.png)
 
1. From the menu on the left side, select **Analyze**, your environment will be loaded with 2 sample datasets **Contoso Plant 1** and **Contoso Plant 2**.

1. From the menu on the left side, select **Model**, and review the available models.

1. From the menu on the left side select **Analyze**.

1. The Time Series Insights data explorer will open in a new tab. By default, a chart displaying a count of all events is displayed. Notice how there is a timeline directly underneath the chart, and a more macro-level timeline below that, showing several hours.
  
1. Remove the existing queries by clicking on the **delete** icon.

   ![](media/tsi01.png)

   >**Note**: To see the delete icon, move the cursor to the left side of the selected query.
      
1. On the left pane, expand Contoso Plant 1 **->** W6 **->** Weather System **->** OutdoorTemperature.

1. Click on the listed data, then select **Reading** and Click on **Add**.

   ![](media/tsi02.png)

1. The line chart will be displayed with the average temperature values accordingly based on specific timespan.

1. From the **Interval** dropdown, change the interval and then observe the changes that are reflected in the graph.

1. Select Settings ( gear icon ) within the new measure you created, then check the box next to Use **Stepped Interpolation** and **Min/Max Shadows**.

1. The chart will update to reflect those changes. Notice how **Show Min/Max** creates a shadow behind the interpolated line to show the minimum and maximum temperature values. This is because the displayed line is the average temperature value.

1. You can use the Interval Size slider tool to zoom in and out of intervals over the same timespan. The slider provides more precise control of movement between large slices of time that show smooth trends down to slices as small as the millisecond, which allows you to display and analyze granular, high-resolution cuts of your data.

   ![](media/tsi06.png)

1. Click on **More Actions** which is present at the top right corner of the environment.

1. From the dropdown, select **Explore raw events**.

   ![](media/tsi08.png)

1. You may select which columns you want to view and export the data if desired by selecting **Download as CSV** which is at the left bottom of the events dialog and you can close.

   ![](media/tsi05.png)
   
1. Then, Click on **More Actions** which is present at the top right corner of the environment, and select **Show chart data as table**.
  
1. A table will be displayed which consists of **maximum**,**average** and **minimum temperature** between specific time intervals.

   ![](media/tsi09.png)

   >**Note**: Please ensure to **Stop** the Stream Analytics Job and navigate to IoT Simulator App. Then click on the **Stop telemetry** button to stop sending the telemetry stream.

In this exercise, you saw how the Stream Analytics job pulls telemetry messages from IoT Hub and sends the messages to two different destinations. One query that retrieves all messages and sends them to Blob Storage (the cold path), and another that selects out only the important events needed for reporting in real-time (the hot path) using Power BI visualizations and reports.
